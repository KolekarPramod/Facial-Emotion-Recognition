{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afc6a52",
   "metadata": {},
   "source": [
    "## Project Name = DeepFER: Facial Emotion Recognition Using Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe23b7e",
   "metadata": {},
   "source": [
    "Project Type - Classification (Deep Learning for Computer Vision)\n",
    "\n",
    "Contribution - Individual\n",
    "\n",
    "Member- Pramod Kolekar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558127e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**ğŸ” Project Summary: DeepFER â€“ Facial Emotion Recognition Using Deep Learning**\n",
    "\n",
    "- **Objective:**  \n",
    "  Develop a robust and real-time facial emotion recognition system using deep learning techniques.\n",
    "\n",
    "- **Core Technologies:**  \n",
    "  - Convolutional Neural Networks (CNNs)  \n",
    "  - Transfer Learning  \n",
    "  - Data Augmentation (rotation, scaling, flipping)\n",
    "\n",
    "- **Emotion Classes Recognized:**  \n",
    "  - Angry  \n",
    "  - Sad  \n",
    "  - Happy  \n",
    "  - Fear  \n",
    "  - Neutral  \n",
    "  - Disgust  \n",
    "  - Surprise\n",
    "\n",
    "- **Dataset Characteristics:**  \n",
    "  - High-quality facial images with diverse backgrounds and lighting  \n",
    "  - Includes both posed and spontaneous expressions  \n",
    "  - Labeled with corresponding emotion annotations  \n",
    "  - Sourced from public databases and crowd-sourced contributions  \n",
    "\n",
    "- **Model Development:**  \n",
    "  - Custom CNN architecture  \n",
    "  - Fine-tuning of pre-trained models for improved accuracy  \n",
    "  - Optimization for precision, recall, F1-score, and real-time processing  \n",
    "\n",
    "- **Applications:**  \n",
    "  - Human-computer interaction  \n",
    "  - Mental health monitoring  \n",
    "  - Customer service enhancement  \n",
    "  - Empathetic AI-driven systems\n",
    "\n",
    "- **Key Goals:**  \n",
    "  - Achieve high emotion classification accuracy  \n",
    "  - Enable real-time emotion detection from video feeds  \n",
    "  - Create a deployable, user-friendly application  \n",
    "\n",
    "- **Project Outputs:**  \n",
    "  - Trained emotion recognition model  \n",
    "  - Real-time emotion detection interface  \n",
    "  - Comprehensive documentation and deployment in real-world scenarios  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcccbd4c-e65a-4608-920c-2c20e67cadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "from tqdm.notebook import tqdm\n",
    "from keras_preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81dcdae-7620-4cf9-930c-96bbbc0d79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c35fa2f-a5a9-466c-a3f3-82872eb0f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "640f36a6-6524-4938-a93a-ebaf334f2c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccd20d02-3f9c-4d31-8a8e-83c82c8dc3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b241a65b-c238-4872-a4aa-f8c6739c9b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f65e5",
   "metadata": {},
   "source": [
    "extract_features(images) is designed to read a list of image file paths, convert each image into a consistent grayscale array format, and reshape them into a form thatâ€™s ready for training or inference in a CNN modelâ€”specifically one expecting images of size 48Ã—48 pixels with a single color channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc7b57-a74f-4cc6-ac06-bfed43f4dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode='grayscale')\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88d6056-fe6f-4a53-bc65-7444c5061377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8452c1ac8cfc404a89c879fd234abfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = extract_features(train['image']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8190c899-84f3-499a-924d-6cbe4e3ad869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58142a1e00e4771aa8ed78448e1be83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f63072d-acb6-402e-9277-ca2d61a56353",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "361335a7-c4df-4548-94a7-f7352702fdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3369d9e1-6717-417b-ba41-b970c3f4fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62b74471-57e7-4d7a-a505-0a48efde9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes = 7)\n",
    "y_test = to_categorical(y_test,num_classes = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b157155",
   "metadata": {},
   "source": [
    "## DL Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "909911dd-b144-4fc5-b70a-5c8af8073902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input\n",
    "\n",
    "model = Sequential()\n",
    "# Add an explicit Input layer\n",
    "model.add(Input(shape=(48,48,1)))\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4, name='dropout_1'))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4, name='dropout_2'))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4, name='dropout_3'))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4, name='dropout_4'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4, name='dropout_5'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3, name='dropout_6'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a22771ca-026f-4381-98ee-5b27ee410e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db65a16f-17a4-4272-b164-fae26ceca17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11af0ad8-91d1-425f-a40a-99aa1163be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 835ms/step - accuracy: 0.2376 - loss: 1.8387 - val_accuracy: 0.2583 - val_loss: 1.8103\n",
      "Epoch 2/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.2462 - loss: 1.8061 - val_accuracy: 0.2884 - val_loss: 1.7290\n",
      "Epoch 3/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 828ms/step - accuracy: 0.2800 - loss: 1.7361 - val_accuracy: 0.3556 - val_loss: 1.6178\n",
      "Epoch 4/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 835ms/step - accuracy: 0.3540 - loss: 1.6266 - val_accuracy: 0.4341 - val_loss: 1.4506\n",
      "Epoch 5/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 866ms/step - accuracy: 0.4124 - loss: 1.5157 - val_accuracy: 0.4867 - val_loss: 1.3445\n",
      "Epoch 6/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.4504 - loss: 1.4277 - val_accuracy: 0.4870 - val_loss: 1.3234\n",
      "Epoch 7/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.4669 - loss: 1.3910 - val_accuracy: 0.5081 - val_loss: 1.2756\n",
      "Epoch 8/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.4751 - loss: 1.3526 - val_accuracy: 0.5245 - val_loss: 1.2452\n",
      "Epoch 9/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.4867 - loss: 1.3416 - val_accuracy: 0.5395 - val_loss: 1.2147\n",
      "Epoch 10/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 832ms/step - accuracy: 0.4981 - loss: 1.2978 - val_accuracy: 0.5466 - val_loss: 1.1961\n",
      "Epoch 11/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 828ms/step - accuracy: 0.5098 - loss: 1.2859 - val_accuracy: 0.5517 - val_loss: 1.1788\n",
      "Epoch 12/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 829ms/step - accuracy: 0.5266 - loss: 1.2515 - val_accuracy: 0.5548 - val_loss: 1.1709\n",
      "Epoch 13/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 825ms/step - accuracy: 0.5260 - loss: 1.2498 - val_accuracy: 0.5584 - val_loss: 1.1627\n",
      "Epoch 14/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.5356 - loss: 1.2290 - val_accuracy: 0.5644 - val_loss: 1.1464\n",
      "Epoch 15/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 829ms/step - accuracy: 0.5407 - loss: 1.2083 - val_accuracy: 0.5695 - val_loss: 1.1343\n",
      "Epoch 16/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 837ms/step - accuracy: 0.5451 - loss: 1.2005 - val_accuracy: 0.5592 - val_loss: 1.1505\n",
      "Epoch 17/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 828ms/step - accuracy: 0.5527 - loss: 1.1754 - val_accuracy: 0.5783 - val_loss: 1.1065\n",
      "Epoch 18/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.5579 - loss: 1.1669 - val_accuracy: 0.5764 - val_loss: 1.1185\n",
      "Epoch 19/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 829ms/step - accuracy: 0.5570 - loss: 1.1649 - val_accuracy: 0.5845 - val_loss: 1.1118\n",
      "Epoch 20/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.5612 - loss: 1.1577 - val_accuracy: 0.5863 - val_loss: 1.0917\n",
      "Epoch 21/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 826ms/step - accuracy: 0.5729 - loss: 1.1278 - val_accuracy: 0.5834 - val_loss: 1.1020\n",
      "Epoch 22/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 830ms/step - accuracy: 0.5788 - loss: 1.1174 - val_accuracy: 0.5889 - val_loss: 1.0878\n",
      "Epoch 23/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 830ms/step - accuracy: 0.5781 - loss: 1.1212 - val_accuracy: 0.5933 - val_loss: 1.0833\n",
      "Epoch 24/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 827ms/step - accuracy: 0.5813 - loss: 1.1056 - val_accuracy: 0.5903 - val_loss: 1.0753\n",
      "Epoch 25/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 830ms/step - accuracy: 0.5772 - loss: 1.1052 - val_accuracy: 0.6027 - val_loss: 1.0693\n",
      "Epoch 26/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 833ms/step - accuracy: 0.5924 - loss: 1.0889 - val_accuracy: 0.5989 - val_loss: 1.0727\n",
      "Epoch 27/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.5868 - loss: 1.0896 - val_accuracy: 0.6039 - val_loss: 1.0589\n",
      "Epoch 28/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 835ms/step - accuracy: 0.5928 - loss: 1.0743 - val_accuracy: 0.6085 - val_loss: 1.0560\n",
      "Epoch 29/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.5968 - loss: 1.0651 - val_accuracy: 0.6037 - val_loss: 1.0561\n",
      "Epoch 30/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 834ms/step - accuracy: 0.5910 - loss: 1.0738 - val_accuracy: 0.6016 - val_loss: 1.0628\n",
      "Epoch 31/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 834ms/step - accuracy: 0.5978 - loss: 1.0575 - val_accuracy: 0.6054 - val_loss: 1.0551\n",
      "Epoch 32/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 834ms/step - accuracy: 0.6114 - loss: 1.0435 - val_accuracy: 0.6054 - val_loss: 1.0518\n",
      "Epoch 33/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.6086 - loss: 1.0341 - val_accuracy: 0.6071 - val_loss: 1.0534\n",
      "Epoch 34/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 837ms/step - accuracy: 0.6125 - loss: 1.0302 - val_accuracy: 0.6105 - val_loss: 1.0343\n",
      "Epoch 35/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 840ms/step - accuracy: 0.6114 - loss: 1.0324 - val_accuracy: 0.6094 - val_loss: 1.0366\n",
      "Epoch 36/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 835ms/step - accuracy: 0.6191 - loss: 1.0163 - val_accuracy: 0.6093 - val_loss: 1.0443\n",
      "Epoch 37/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.6182 - loss: 1.0128 - val_accuracy: 0.6124 - val_loss: 1.0415\n",
      "Epoch 38/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 830ms/step - accuracy: 0.6203 - loss: 0.9998 - val_accuracy: 0.6090 - val_loss: 1.0445\n",
      "Epoch 39/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.6268 - loss: 0.9978 - val_accuracy: 0.6081 - val_loss: 1.0495\n",
      "Epoch 40/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 831ms/step - accuracy: 0.6275 - loss: 0.9873 - val_accuracy: 0.6054 - val_loss: 1.0512\n",
      "Epoch 41/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 2s/step - accuracy: 0.6255 - loss: 0.9954 - val_accuracy: 0.6162 - val_loss: 1.0253\n",
      "Epoch 42/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 2s/step - accuracy: 0.6405 - loss: 0.9606 - val_accuracy: 0.6139 - val_loss: 1.0443\n",
      "Epoch 43/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 2s/step - accuracy: 0.6335 - loss: 0.9733 - val_accuracy: 0.6149 - val_loss: 1.0379\n",
      "Epoch 44/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 2s/step - accuracy: 0.6381 - loss: 0.9651 - val_accuracy: 0.6141 - val_loss: 1.0374\n",
      "Epoch 45/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 2s/step - accuracy: 0.6345 - loss: 0.9746 - val_accuracy: 0.6110 - val_loss: 1.0416\n",
      "Epoch 46/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 2s/step - accuracy: 0.6399 - loss: 0.9570 - val_accuracy: 0.6183 - val_loss: 1.0292\n",
      "Epoch 47/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34383s\u001b[0m 153s/step - accuracy: 0.6542 - loss: 0.9415 - val_accuracy: 0.6128 - val_loss: 1.0377\n",
      "Epoch 48/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 824ms/step - accuracy: 0.6435 - loss: 0.9445 - val_accuracy: 0.6220 - val_loss: 1.0371\n",
      "Epoch 49/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 869ms/step - accuracy: 0.6476 - loss: 0.9399 - val_accuracy: 0.6190 - val_loss: 1.0291\n",
      "Epoch 50/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 813ms/step - accuracy: 0.6502 - loss: 0.9305 - val_accuracy: 0.6214 - val_loss: 1.0260\n",
      "Epoch 51/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 825ms/step - accuracy: 0.6565 - loss: 0.9216 - val_accuracy: 0.6203 - val_loss: 1.0315\n",
      "Epoch 52/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 814ms/step - accuracy: 0.6434 - loss: 0.9474 - val_accuracy: 0.6217 - val_loss: 1.0231\n",
      "Epoch 53/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 815ms/step - accuracy: 0.6577 - loss: 0.9162 - val_accuracy: 0.6284 - val_loss: 1.0318\n",
      "Epoch 54/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 815ms/step - accuracy: 0.6606 - loss: 0.9054 - val_accuracy: 0.6216 - val_loss: 1.0314\n",
      "Epoch 55/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 810ms/step - accuracy: 0.6570 - loss: 0.9185 - val_accuracy: 0.6255 - val_loss: 1.0301\n",
      "Epoch 56/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 805ms/step - accuracy: 0.6631 - loss: 0.9006 - val_accuracy: 0.6211 - val_loss: 1.0273\n",
      "Epoch 57/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 807ms/step - accuracy: 0.6592 - loss: 0.9065 - val_accuracy: 0.6241 - val_loss: 1.0265\n",
      "Epoch 58/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 810ms/step - accuracy: 0.6719 - loss: 0.8858 - val_accuracy: 0.6187 - val_loss: 1.0440\n",
      "Epoch 59/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.6649 - loss: 0.8956 - val_accuracy: 0.6261 - val_loss: 1.0269\n",
      "Epoch 60/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 851ms/step - accuracy: 0.6691 - loss: 0.8848 - val_accuracy: 0.6200 - val_loss: 1.0369\n",
      "Epoch 61/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 821ms/step - accuracy: 0.6708 - loss: 0.8870 - val_accuracy: 0.6312 - val_loss: 1.0255\n",
      "Epoch 62/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 812ms/step - accuracy: 0.6740 - loss: 0.8700 - val_accuracy: 0.6180 - val_loss: 1.0387\n",
      "Epoch 63/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 812ms/step - accuracy: 0.6788 - loss: 0.8666 - val_accuracy: 0.6226 - val_loss: 1.0323\n",
      "Epoch 64/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 811ms/step - accuracy: 0.6798 - loss: 0.8661 - val_accuracy: 0.6258 - val_loss: 1.0284\n",
      "Epoch 65/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 820ms/step - accuracy: 0.6864 - loss: 0.8408 - val_accuracy: 0.6265 - val_loss: 1.0318\n",
      "Epoch 66/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 814ms/step - accuracy: 0.6842 - loss: 0.8500 - val_accuracy: 0.6275 - val_loss: 1.0198\n",
      "Epoch 67/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 839ms/step - accuracy: 0.6867 - loss: 0.8500 - val_accuracy: 0.6233 - val_loss: 1.0453\n",
      "Epoch 68/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 820ms/step - accuracy: 0.6860 - loss: 0.8446 - val_accuracy: 0.6323 - val_loss: 1.0283\n",
      "Epoch 69/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 821ms/step - accuracy: 0.6938 - loss: 0.8356 - val_accuracy: 0.6294 - val_loss: 1.0275\n",
      "Epoch 70/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 826ms/step - accuracy: 0.6861 - loss: 0.8431 - val_accuracy: 0.6258 - val_loss: 1.0296\n",
      "Epoch 71/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 813ms/step - accuracy: 0.6927 - loss: 0.8309 - val_accuracy: 0.6279 - val_loss: 1.0322\n",
      "Epoch 72/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 815ms/step - accuracy: 0.6953 - loss: 0.8267 - val_accuracy: 0.6272 - val_loss: 1.0326\n",
      "Epoch 73/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 813ms/step - accuracy: 0.6926 - loss: 0.8241 - val_accuracy: 0.6298 - val_loss: 1.0426\n",
      "Epoch 74/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 832ms/step - accuracy: 0.6985 - loss: 0.8139 - val_accuracy: 0.6282 - val_loss: 1.0273\n",
      "Epoch 75/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 818ms/step - accuracy: 0.6971 - loss: 0.8203 - val_accuracy: 0.6277 - val_loss: 1.0396\n",
      "Epoch 76/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 817ms/step - accuracy: 0.7011 - loss: 0.8120 - val_accuracy: 0.6294 - val_loss: 1.0236\n",
      "Epoch 77/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 878ms/step - accuracy: 0.7006 - loss: 0.8019 - val_accuracy: 0.6305 - val_loss: 1.0299\n",
      "Epoch 78/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 856ms/step - accuracy: 0.7147 - loss: 0.7871 - val_accuracy: 0.6241 - val_loss: 1.0318\n",
      "Epoch 79/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 2s/step - accuracy: 0.7068 - loss: 0.7933 - val_accuracy: 0.6238 - val_loss: 1.0452\n",
      "Epoch 80/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34005s\u001b[0m 151s/step - accuracy: 0.7059 - loss: 0.7883 - val_accuracy: 0.6316 - val_loss: 1.0225\n",
      "Epoch 81/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 867ms/step - accuracy: 0.7123 - loss: 0.7819 - val_accuracy: 0.6335 - val_loss: 1.0306\n",
      "Epoch 82/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 870ms/step - accuracy: 0.7161 - loss: 0.7674 - val_accuracy: 0.6316 - val_loss: 1.0252\n",
      "Epoch 83/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 868ms/step - accuracy: 0.7113 - loss: 0.7964 - val_accuracy: 0.6330 - val_loss: 1.0187\n",
      "Epoch 84/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 862ms/step - accuracy: 0.7182 - loss: 0.7781 - val_accuracy: 0.6335 - val_loss: 1.0340\n",
      "Epoch 85/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 872ms/step - accuracy: 0.7183 - loss: 0.7642 - val_accuracy: 0.6272 - val_loss: 1.0553\n",
      "Epoch 86/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 869ms/step - accuracy: 0.7172 - loss: 0.7610 - val_accuracy: 0.6320 - val_loss: 1.0269\n",
      "Epoch 87/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 868ms/step - accuracy: 0.7223 - loss: 0.7643 - val_accuracy: 0.6335 - val_loss: 1.0298\n",
      "Epoch 88/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 861ms/step - accuracy: 0.7233 - loss: 0.7554 - val_accuracy: 0.6315 - val_loss: 1.0263\n",
      "Epoch 89/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 866ms/step - accuracy: 0.7257 - loss: 0.7457 - val_accuracy: 0.6363 - val_loss: 1.0440\n",
      "Epoch 90/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 861ms/step - accuracy: 0.7323 - loss: 0.7430 - val_accuracy: 0.6343 - val_loss: 1.0262\n",
      "Epoch 91/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 866ms/step - accuracy: 0.7304 - loss: 0.7398 - val_accuracy: 0.6397 - val_loss: 1.0250\n",
      "Epoch 92/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 865ms/step - accuracy: 0.7317 - loss: 0.7330 - val_accuracy: 0.6395 - val_loss: 1.0384\n",
      "Epoch 93/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 862ms/step - accuracy: 0.7302 - loss: 0.7347 - val_accuracy: 0.6394 - val_loss: 1.0262\n",
      "Epoch 94/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 867ms/step - accuracy: 0.7347 - loss: 0.7246 - val_accuracy: 0.6332 - val_loss: 1.0213\n",
      "Epoch 95/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 863ms/step - accuracy: 0.7344 - loss: 0.7234 - val_accuracy: 0.6374 - val_loss: 1.0367\n",
      "Epoch 96/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 861ms/step - accuracy: 0.7358 - loss: 0.7261 - val_accuracy: 0.6364 - val_loss: 1.0344\n",
      "Epoch 97/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 2s/step - accuracy: 0.7331 - loss: 0.7298 - val_accuracy: 0.6302 - val_loss: 1.0287\n",
      "Epoch 98/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 2s/step - accuracy: 0.7385 - loss: 0.7126 - val_accuracy: 0.6391 - val_loss: 1.0417\n",
      "Epoch 99/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 2s/step - accuracy: 0.7388 - loss: 0.7214 - val_accuracy: 0.6357 - val_loss: 1.0302\n",
      "Epoch 100/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 2s/step - accuracy: 0.7415 - loss: 0.7113 - val_accuracy: 0.6378 - val_loss: 1.0372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d65bcf3470>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= x_train,y = y_train, batch_size = 128, epochs = 100, validation_data = (x_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e718484-aea0-4e94-be5a-0b7e700cd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in Keras format (.keras extension)\n",
    "model.save(\"emotiondetector.keras\")  # Recommended Keras format\n",
    "\n",
    "# Optionally, you can still save the model's architecture to a JSON file, if needed\n",
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34f1f01e-c6e5-4f32-be02-9b2313438000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "\n",
    "# Define the path to the .keras file\n",
    "file_path = \"emotiondetec.keras\"\n",
    "\n",
    "# Check if the file exists before attempting to load it\n",
    "if os.path.exists(file_path):\n",
    "    # Load the model directly from the .keras file\n",
    "    model = load_model(file_path)\n",
    "    \n",
    "    # Recompile the model with the desired optimizer\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Model loaded and compiled successfully.\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}. Please ensure the file path is correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38d2785d-ce36-4f10-a232-d3888a6a4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30665c03-0237-473f-8914-1dadcd47b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ef(image_path):\n",
    "    # Load image with color_mode='grayscale'\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(48, 48))\n",
    "    img_array = img_to_array(img)\n",
    "    # Normalize the image\n",
    "    img_array = img_array / 255.0\n",
    "    # Reshape for model input\n",
    "    img_array = img_array.reshape(1, 48, 48, 1)\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54466d01-c70b-4ad9-8788-e2e0d05eaade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image is of sad\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927ms/step\n",
      "Model prediction is  sad\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Path to the image\n",
    "image_path = 'images/train/sad/42.jpg'\n",
    "print(\"Original image is of sad\")\n",
    "\n",
    "# Extract features\n",
    "img = ef(image_path)\n",
    "\n",
    "# Predict using the model\n",
    "pred = model.predict(img)\n",
    "pred_label = labels[pred.argmax()]  # Ensure 'labels' is defined\n",
    "\n",
    "print(\"Model prediction is \", pred_label)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img.reshape(48, 48), cmap='gray')\n",
    "plt.title(f'Predicted: {pred_label}')\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a03212d-7cea-4d5d-bb17-9ac4ee3bbff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7066 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\work\\Facial-Emotion-Recognition\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m221/221\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 544ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.57      0.52      0.54       960\n",
      "     disgust       0.68      0.59      0.63       111\n",
      "        fear       0.54      0.37      0.44      1018\n",
      "       happy       0.80      0.86      0.82      1825\n",
      "     neutral       0.53      0.62      0.57      1216\n",
      "         sad       0.50      0.53      0.52      1139\n",
      "    surprise       0.77      0.78      0.78       797\n",
      "\n",
      "    accuracy                           0.63      7066\n",
      "   macro avg       0.63      0.61      0.61      7066\n",
      "weighted avg       0.63      0.63      0.63      7066\n",
      "\n",
      "Accuracy: 0.6338805547693178\n",
      "Precision: 0.6301398972268686\n",
      "Recall: 0.6338805547693178\n",
      "F1 Score: 0.6284740847856805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the paths to the training and test data\n",
    "train_data_dir = 'D:/work/Facial-Emotion-Recognition/images/train'\n",
    "test_data_dir = 'D:/work/Facial-Emotion-Recognition/images/test'\n",
    "img_size = (48, 48)  # Size expected by your model\n",
    "\n",
    "# Create an image data generator for the test set\n",
    "datagen = ImageDataGenerator(rescale=1./255)  # Normalize images\n",
    "\n",
    "# Load the test dataset from the directory\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=img_size,\n",
    "    color_mode=\"grayscale\",  # As your model expects grayscale images\n",
    "    batch_size=32,  # You can adjust the batch size as needed\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Predict the model's output on the test set\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Get the predicted class indices\n",
    "\n",
    "# Get the true labels from the test data\n",
    "true_classes = test_generator.classes  # Ground truth labels\n",
    "\n",
    "# Get class labels (emotion names) from the generator\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e4d3711-f966-4a46-b5c7-32cff0440e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Improve Data Quality and Quantity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21bfc332-b100-4b1d-9112-f7551b98c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fecb01-ae2f-4c6b-a60b-2f6a5ccb46ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
